{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meetra21/Contour__Gradient_pipeline/blob/main/Contour__Gradient_pipeline_for_Digits_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c95940",
      "metadata": {
        "id": "b4c95940"
      },
      "outputs": [],
      "source": [
        "# If you need these in Colab, uncomment:\n",
        "# !pip install --quiet tensorflow tensorflow-datasets opencv-python tqdm scikit-learn\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Boundary + central-diff gradients (+ magnitude)\n",
        "# -----------------------------\n",
        "def _auto_canny_edges_uint8(img_uint8, sigma=0.33):\n",
        "    v = np.median(img_uint8)\n",
        "    lower = int(max(0, (1.0 - sigma) * v))\n",
        "    upper = int(min(255, (1.0 + sigma) * v))\n",
        "    return cv2.Canny(img_uint8, lower, upper, L2gradient=True)\n",
        "\n",
        "def compute_boundary_gradients(images, use_canny=True, add_mag=True):\n",
        "    \"\"\"\n",
        "    images: (N, 28, 28) uint8 or float\n",
        "    Returns: (N, C, 28, 28) float32 where C=2 (gx,gy) or 3 (gx,gy,mag)\n",
        "    - central-difference grads (vectorized)\n",
        "    - masked to edges (Canny) to emphasize contours\n",
        "    - per-image L2 normalization to stabilize scale\n",
        "    \"\"\"\n",
        "    imgs = images.astype(np.float32)\n",
        "    if imgs.max() > 1.0:\n",
        "        imgs /= 255.0\n",
        "\n",
        "    N, H, W = imgs.shape\n",
        "    p = np.pad(imgs, ((0,0),(1,1),(1,1)), mode='edge')\n",
        "\n",
        "    gx = (p[:, 1:-1, 2:] - p[:, 1:-1, :-2]) * 0.5\n",
        "    gy = (p[:, 2:, 1:-1] - p[:, :-2, 1:-1]) * 0.5\n",
        "\n",
        "    if use_canny:\n",
        "        edges = np.empty_like(images, dtype=np.uint8)\n",
        "        imgs_u8 = (imgs * 255.0 + 0.5).astype(np.uint8)\n",
        "        for i in range(N):\n",
        "            edges[i] = _auto_canny_edges_uint8(imgs_u8[i])\n",
        "        mask = (edges > 0).astype(np.float32)\n",
        "    else:\n",
        "        # fallback boundary proxy (morph gradient)\n",
        "        kernel = np.ones((3,3), np.uint8)\n",
        "        imgs_u8 = (imgs * 255.0 + 0.5).astype(np.uint8)\n",
        "        dil = np.stack([cv2.dilate(im, kernel, iterations=1) for im in imgs_u8], axis=0)\n",
        "        ero = np.stack([cv2.erode(im, kernel, iterations=1) for im in imgs_u8], axis=0)\n",
        "        mask = ((dil - ero) > 0).astype(np.float32)\n",
        "\n",
        "    gx *= mask\n",
        "    gy *= mask\n",
        "\n",
        "    eps = 1e-6\n",
        "    norm = np.sqrt((gx**2 + gy**2).sum(axis=(1,2), keepdims=True)) + eps\n",
        "    gx /= norm\n",
        "    gy /= norm\n",
        "\n",
        "    if add_mag:\n",
        "        mag = np.sqrt(gx**2 + gy**2)\n",
        "        feats = np.stack([gx, gy, mag], axis=1).astype(np.float32)  # (N,3,H,W)\n",
        "    else:\n",
        "        feats = np.stack([gx, gy], axis=1).astype(np.float32)       # (N,2,H,W)\n",
        "\n",
        "    return feats\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Load MNIST via TFDS\n",
        "# -----------------------------\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train', 'test'],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Convert to NumPy arrays\n",
        "# -----------------------------\n",
        "all_images, all_labels = [], []\n",
        "for img, lbl in tfds.as_numpy(ds_train.concatenate(ds_test)):\n",
        "    all_images.append(img.squeeze())   # (28,28) uint8\n",
        "    all_labels.append(int(lbl))        # 0..9\n",
        "all_images = np.stack(all_images)      # (70000,28,28)\n",
        "all_labels = np.array(all_labels, dtype=np.int32)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Stratified 80/20 split\n",
        "# -----------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    all_images, all_labels,\n",
        "    train_size=0.8,\n",
        "    stratify=all_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Boundary+central-diff grads (+mag)\n",
        "# -----------------------------\n",
        "grad_train = compute_boundary_gradients(X_train, use_canny=True, add_mag=True)  # (N,3,28,28)\n",
        "grad_test  = compute_boundary_gradients(X_test,  use_canny=True, add_mag=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Flatten + global z-score standardization\n",
        "# -----------------------------\n",
        "C = grad_train.shape[1]\n",
        "X_train_feat = grad_train.reshape(-1, C*28*28)\n",
        "X_test_feat  = grad_test.reshape(-1,  C*28*28)\n",
        "\n",
        "mean = X_train_feat.mean(axis=0, keepdims=True)\n",
        "std  = X_train_feat.std(axis=0, keepdims=True) + 1e-6\n",
        "X_train_feat = (X_train_feat - mean) / std\n",
        "X_test_feat  = (X_test_feat  - mean) / std\n",
        "\n",
        "# -----------------------------\n",
        "# 7) MLP model (accuracy-leaning but fast)\n",
        "# -----------------------------\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(C*28*28,)),\n",
        "    layers.Dense(1024), layers.BatchNormalization(), layers.LeakyReLU(alpha=0.01),\n",
        "    layers.Dropout(0.4),\n",
        "\n",
        "    layers.Dense(512), layers.BatchNormalization(), layers.LeakyReLU(alpha=0.01),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    layers.Dense(256), layers.BatchNormalization(), layers.LeakyReLU(alpha=0.01),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 8) Callbacks\n",
        "# -----------------------------\n",
        "es = callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy', patience=6,\n",
        "    restore_best_weights=True, verbose=1\n",
        ")\n",
        "rlr = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.5,\n",
        "    patience=3, min_lr=1e-6, verbose=1\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 9) Train\n",
        "# -----------------------------\n",
        "t0 = time.time()\n",
        "history = model.fit(\n",
        "    X_train_feat, y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=60,\n",
        "    batch_size=256,                 # larger batch; features are small\n",
        "    callbacks=[es, rlr],\n",
        "    verbose=2\n",
        ")\n",
        "train_time = time.time() - t0\n",
        "\n",
        "# -----------------------------\n",
        "# 10) Evaluate\n",
        "# -----------------------------\n",
        "t1 = time.time()\n",
        "test_loss, test_acc = model.evaluate(X_test_feat, y_test, verbose=0)\n",
        "infer_time = time.time() - t1\n",
        "\n",
        "# -----------------------------\n",
        "# 11) Report\n",
        "# -----------------------------\n",
        "print(f\"\\nMNIST Boundary-Masked Central-Gradient (+Mag) MLP\")\n",
        "print(f\"Input Channels  : {C}\")\n",
        "print(f\"Test Accuracy   : {test_acc*100:.2f}%\")\n",
        "print(f\"Training Time   : {train_time:.1f}s\")\n",
        "print(f\"Inference Time  : {infer_time:.3f}s for {X_test_feat.shape[0]} samples\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}