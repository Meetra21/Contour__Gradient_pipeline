{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meetra21/Contour__Gradient_pipeline/blob/main/Contour__Gradient_pipeline_for_Letters_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f99fa641",
      "metadata": {
        "id": "f99fa641"
      },
      "outputs": [],
      "source": [
        "# If you need these in Colab, uncomment:\n",
        "# !pip install --quiet tensorflow tensorflow-datasets opencv-python scikit-learn tqdm\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Boundary+Gradient features\n",
        "# -----------------------------\n",
        "def _auto_canny_edges_uint8(img_uint8, sigma=0.33):\n",
        "    \"\"\"\n",
        "    Fast auto-threshold Canny for a single uint8 image.\n",
        "    Returns a binary edge map (uint8 {0,255}).\n",
        "    \"\"\"\n",
        "    v = np.median(img_uint8)\n",
        "    lower = int(max(0, (1.0 - sigma) * v))\n",
        "    upper = int(min(255, (1.0 + sigma) * v))\n",
        "    return cv2.Canny(img_uint8, lower, upper, L2gradient=True)\n",
        "\n",
        "def compute_boundary_gradients(images, use_canny=True):\n",
        "    \"\"\"\n",
        "    images: (N, 28, 28) uint8 or float\n",
        "    Returns grads: (N, 2, 28, 28) with gx, gy computed by central differences,\n",
        "    masked to the boundary (Canny edges). Outside edges -> 0.\n",
        "    - Vectorized gradient (no Python loop over gradient).\n",
        "    - Minimal loop only for Canny (fast at this scale).\n",
        "    \"\"\"\n",
        "    # Normalize to [0,1] float32\n",
        "    imgs = images.astype(np.float32)\n",
        "    if imgs.max() > 1.0:\n",
        "        imgs /= 255.0\n",
        "\n",
        "    N, H, W = imgs.shape\n",
        "    # Pad once for vectorized central differences\n",
        "    p = np.pad(imgs, ((0,0),(1,1),(1,1)), mode='edge')\n",
        "\n",
        "    # Central differences (vectorized)\n",
        "    gx = (p[:, 1:-1, 2:] - p[:, 1:-1, :-2]) * 0.5\n",
        "    gy = (p[:, 2:, 1:-1] - p[:, :-2, 1:-1]) * 0.5\n",
        "\n",
        "    # Optional: forward/backward at borders already handled via pad('edge')\n",
        "\n",
        "    # Edge mask via Canny on uint8 source (loop but fast)\n",
        "    if use_canny:\n",
        "        edges = np.empty_like(images, dtype=np.uint8)\n",
        "        # ensure uint8 0..255 for canny\n",
        "        imgs_u8 = (imgs * 255.0 + 0.5).astype(np.uint8)\n",
        "        for i in range(N):\n",
        "            edges[i] = _auto_canny_edges_uint8(imgs_u8[i])\n",
        "        # convert to {0,1} mask and broadcast\n",
        "        mask = (edges > 0).astype(np.float32)\n",
        "    else:\n",
        "        # fallback: morphological gradient as boundary proxy (no loop)\n",
        "        # (dilation - erosion)\n",
        "        kernel = np.ones((3,3), np.uint8)\n",
        "        imgs_u8 = (imgs * 255.0 + 0.5).astype(np.uint8)\n",
        "        dil = np.stack([cv2.dilate(im, kernel, iterations=1) for im in imgs_u8], axis=0)\n",
        "        ero = np.stack([cv2.erode(im, kernel, iterations=1) for im in imgs_u8], axis=0)\n",
        "        mask = ((dil - ero) > 0).astype(np.float32)\n",
        "\n",
        "    # Mask gradients to boundary only\n",
        "    gx *= mask\n",
        "    gy *= mask\n",
        "\n",
        "    # Per-image normalization (robust) to reduce scale variance\n",
        "    # Use L2 over boundary pixels; add eps to avoid div-by-zero for blank edges\n",
        "    eps = 1e-6\n",
        "    norm = np.sqrt((gx**2 + gy**2).sum(axis=(1,2), keepdims=True)) + eps\n",
        "    gx /= norm\n",
        "    gy /= norm\n",
        "\n",
        "    # Stack as (N, 2, H, W) float32\n",
        "    grads = np.stack([gx, gy], axis=1).astype(np.float32)\n",
        "    return grads\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Load EMNIST Letters\n",
        "# -----------------------------\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'emnist/letters',\n",
        "    split=['train', 'test'],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Convert to NumPy arrays\n",
        "# -----------------------------\n",
        "all_images = []\n",
        "all_labels = []\n",
        "for img, lbl in tfds.as_numpy(ds_train.concatenate(ds_test)):\n",
        "    all_images.append(img.squeeze())        # (28,28) uint8\n",
        "    all_labels.append(int(lbl) - 1)         # labels 1–26 -> 0–25\n",
        "all_images = np.stack(all_images)           # (145600,28,28)\n",
        "all_labels = np.array(all_labels, dtype=np.int32)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Stratified 80/20 split\n",
        "# -----------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    all_images, all_labels,\n",
        "    train_size=0.8,\n",
        "    stratify=all_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Boundary+Central-Diff grads\n",
        "# -----------------------------\n",
        "grad_train = compute_boundary_gradients(X_train, use_canny=True)\n",
        "grad_test  = compute_boundary_gradients(X_test,  use_canny=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Flatten + standardize\n",
        "# -----------------------------\n",
        "X_train_feat = grad_train.reshape(-1, 2*28*28)\n",
        "X_test_feat  = grad_test.reshape(-1, 2*28*28)\n",
        "\n",
        "# Feature standardization (z-score) helps MLP convergence\n",
        "mean = X_train_feat.mean(axis=0, keepdims=True)\n",
        "std  = X_train_feat.std(axis=0, keepdims=True) + 1e-6\n",
        "X_train_feat = (X_train_feat - mean) / std\n",
        "X_test_feat  = (X_test_feat  - mean) / std\n",
        "\n",
        "# -----------------------------\n",
        "# 7) MLP model (lean & fast)\n",
        "# -----------------------------\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(2*28*28,)),\n",
        "    layers.Dense(768), layers.BatchNormalization(), layers.Activation('relu'),\n",
        "    layers.Dropout(0.4),\n",
        "\n",
        "    layers.Dense(384), layers.BatchNormalization(), layers.Activation('relu'),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    layers.Dense(192), layers.BatchNormalization(), layers.Activation('relu'),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(26, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 8) Callbacks\n",
        "# -----------------------------\n",
        "es = callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy', patience=5,\n",
        "    restore_best_weights=True, verbose=1\n",
        ")\n",
        "rlr = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.5,\n",
        "    patience=3, min_lr=1e-6, verbose=1\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 9) Train\n",
        "# -----------------------------\n",
        "t0 = time.time()\n",
        "history = model.fit(\n",
        "    X_train_feat, y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=50,\n",
        "    batch_size=256,             # a bit larger batch (features are small)\n",
        "    callbacks=[es, rlr],\n",
        "    verbose=2\n",
        ")\n",
        "train_time = time.time() - t0\n",
        "\n",
        "# -----------------------------\n",
        "# 10) Evaluate\n",
        "# -----------------------------\n",
        "t1 = time.time()\n",
        "test_loss, test_acc = model.evaluate(X_test_feat, y_test, verbose=0)\n",
        "infer_time = time.time() - t1\n",
        "\n",
        "# -----------------------------\n",
        "# 11) Report\n",
        "# -----------------------------\n",
        "print(f\"\\nEMNIST Letters Boundary-Masked Central-Gradient MLP\")\n",
        "print(f\"Test Accuracy   : {test_acc*100:.2f}%\")\n",
        "print(f\"Training Time   : {train_time:.1f}s\")\n",
        "print(f\"Inference Time  : {infer_time:.3f}s for {X_test_feat.shape[0]} samples\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}